{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path('../dataset/prompts_dataset').glob('*.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths, vocab_size=30_522, min_frequency=2, special_tokens= [\n",
    "    '<s>', '</s>', '<pad>', '<unk>', '<mask>', '<lvl1>', '</lvl1>', '<lvl2>', '</lvl2>', '<lvl3>', '</lvl3>'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model('../dataset/tokenizer/neublla_codex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Open word and search about palastine\").tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embed = tf.keras.layers.Embedding(tokenizer.get_vocab_size(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one input sample text preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Open Word and write in it summerization about palastine\"\n",
    "\n",
    "sample_text_encode = tokenizer.encode(sample_text)\n",
    "sample_text_tokens = sample_text_encode.tokens\n",
    "sample_text_tokens_ids = sample_text_encode.ids\n",
    "sample_text_tokens_seq = np.array(sample_text_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample text encoding info\")\n",
    "print(sample_text_encode)\n",
    "print(\"Sample text tokens\")\n",
    "print(sample_text_tokens)\n",
    "print(\"Sample text tokens ids\")\n",
    "print(sample_text_tokens_ids)\n",
    "print(\"Sample text tokens seq\")\n",
    "print(sample_text_tokens_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embed = tf.keras.layers.Embedding(tokenizer.get_vocab_size(), 4)\n",
    "token_embeddngs = token_embed(sample_text_tokens_seq)\n",
    "\n",
    "print(\"Embedding for the sample text : \", sample_text)\n",
    "print(token_embeddngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequnce_length = 256\n",
    "positional_embedding = tf.keras.layers.Embedding(max_sequnce_length, 4)\n",
    "\n",
    "position_index = tf.range(len(sample_text_tokens_seq))\n",
    "print(position_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embeddings = positional_embedding(position_index)\n",
    "print(\"Position embeddings for the input sequence \\n\", positional_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = token_embeddngs + positional_embeddings\n",
    "print(\"Input to the initial encoder block : \\n\", input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch input preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "input_batch = [\n",
    "    'Open Word and save it in \"Home directory\" as \"my_word_file\"',\n",
    "    'Connect the wifi to \"Aizen-sama\" network',\n",
    "    'Search about palastine new today and give me a summary about it'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the bache\n",
    "input_batch_encodeing = tokenizer.encode_batch(input_batch)\n",
    "\n",
    "# input sequences\n",
    "input_seqs = []\n",
    "\n",
    "input_seqs.append(input_batch_encodeing[0].ids)\n",
    "input_seqs.append(input_batch_encodeing[1].ids)\n",
    "input_seqs.append(input_batch_encodeing[2].ids)\n",
    "\n",
    "print(\"Vectorized inputs : \\n\")\n",
    "print(input_seqs)\n",
    "\n",
    "# padding the inputs to be in the same length\n",
    "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding=\"post\")\n",
    "print(\"input to the encoder is : \")\n",
    "print(padded_input_seqs.shape)\n",
    "print(padded_input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_mask = tf.cast(tf.math.not_equal(padded_input_seqs, 0), tf.float32)\n",
    "print('padded input : ')\n",
    "print(padded_input_seqs, '\\n')\n",
    "print(\"Encoder mask : \")\n",
    "print(encoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded dimenstion of the mask\n",
    "encoder_mask = encoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "encoder_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Self-Attention\n",
    "\n",
    "Q => Queries <br>\n",
    "K => Keysz   <br>\n",
    "V => Values  <br>\n",
    "\n",
    "Attention (Q, K, V) = softmax( (Q* K**T) / (sqrt(dimension_of_K) ) ) * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "\n",
    "    key_dimension = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dimension)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
    "    \n",
    "    softmax = tf.keras.layers.Softmax()\n",
    "    weights = softmax(scaled_scores)\n",
    "\n",
    "    return tf.matmul(weights, value), weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 3\n",
    "embed_dim = 4\n",
    "\n",
    "queries = np.random.rand(seq_len, embed_dim)\n",
    "keys = np.random.rand(seq_len, embed_dim)\n",
    "values = np.random.rand(seq_len, embed_dim)\n",
    "\n",
    "print(\"Queries:\\n\", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attn_weights = scaled_dot_product_attention(queries, keys, values)\n",
    "\n",
    "print(\"Output\\n\", output, \"\\n\")\n",
    "print(\"Weights\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadSelfAttention, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    self.d_head = self.d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
    "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
    "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
    "  \n",
    "  def split_heads(self, x):\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
    "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  def merge_heads(self, x):\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
    "\n",
    "  def call(self, q, k, v, mask):\n",
    "    qs = self.wq(q)\n",
    "    ks = self.wk(k)\n",
    "    vs = self.wv(v)\n",
    "\n",
    "    qs = self.split_heads(qs)\n",
    "    ks = self.split_heads(ks)\n",
    "    vs = self.split_heads(vs)\n",
    "\n",
    "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
    "    output = self.merge_heads(output)\n",
    "\n",
    "    return self.dense(output), attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 3\n",
    "embed_dim = 12\n",
    "num_heads = 3\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "print(f\"Dimension of each head: {head_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
    "print(\"Input shape: \", x.shape, \"\\n\")\n",
    "print(\"Input:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhsa = MultiHeadSelfAttention(12, 3)\n",
    "\n",
    "output, attn_weights = mhsa(x, x, x, None)\n",
    "print(f\"MHSA output{output.shape}:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_network(dimension_model, hidden_dimension):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_dimension, activation='relu'),\n",
    "        tf.keras.layers.Dense(dimension_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dimension_model, num_heads, hidden_dimension, dropout_rate=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.mhsa = MultiHeadSelfAttention(dimension_model, num_heads)\n",
    "        self.ffn = feed_forward_network(dimension_model, hidden_dimension)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        mhsa_output, attention_weights = self.mhsa(x, x, x, mask)\n",
    "        # drop out\n",
    "        mhsa_output = self.dropout1(mhsa_output, training=training)\n",
    "        # skip connection\n",
    "        mhsa_output = self.layernorm1(x + mhsa_output)\n",
    "\n",
    "        ffn_output = self.ffn(mhsa_output)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        output = self.layernorm2(mhsa_output + ffn_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_blocks, dimension_model, num_heads, hidden_dimension, src_vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dimension_model = dimension_model\n",
    "        self.max_sql_len = max_seq_len\n",
    "\n",
    "        self.token_embedding = tf.keras.layers.Embedding(src_vocab_size, self.dimension_model)\n",
    "        self.positonal_embedding = tf.keras.layers.Embedding(max_seq_len, self.dimension_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.blocks = [EncoderBlock(self.dimension_model, num_heads, hidden_dimension, dropout_rate)\n",
    "                       for _ in range(num_blocks)]\n",
    "    \n",
    "    def call(self, input, training, mask):\n",
    "        token_embeddings = self.token_embedding(input)\n",
    "\n",
    "        num_pos = input.shape[0] * self.max_sql_len\n",
    "        positional_index = np.resize(np.arange(self.max_sql_len), num_pos)\n",
    "        positional_index = np.reshape(positional_index, input.shape)\n",
    "        positional_embeddings = self.positonal_embedding(positional_index)\n",
    "\n",
    "        x = self.dropout(token_embeddings + positional_embeddings, training=training)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, weights = block(x, training, mask)\n",
    "        \n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = 6\n",
    "\n",
    "dimension_model = 12\n",
    "\n",
    "num_heads = 3\n",
    "\n",
    "hidden_dimension = 48\n",
    "\n",
    "src_vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "max_seq_len = padded_input_seqs.shape[1]\n",
    "\n",
    "encoder = Encoder(\n",
    "    num_blocks,\n",
    "    dimension_model,\n",
    "    num_heads,\n",
    "    hidden_dimension,\n",
    "    src_vocab_size,\n",
    "    max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output, attn_wieghts = encoder(input=padded_input_seqs, training=True, mask=encoder_mask)\n",
    "\n",
    "print(f\"Encoder Output {encoder_output.shape}:\")\n",
    "print(encoder_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
